{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Installing Transformers and Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Summarization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Summarize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE = \"\"\"\n",
    "You don’t always have to give your boss the finger\n",
    "Maybe it’s your first day on the job. Perhaps your manager just made an announcement. You’ve been asked to scan your fingerprint every time you clock in and out. Is that even allowed?\n",
    "From Hooters to Hyatt Hotels, employers tantalized by the promise of a futuristic, streamlined way to track workers’ attendance are starting to use time clock machines that fingerprint employees.\n",
    "Vendors like Kronos and Allied Time say that because the machines are tied to your biometric information — unique characteristics such as your face, fingerprints, how you talk, and even how you walk — they provide a higher level of workplace security and limit employees’ ability to commit “time theft” by punching in for one another.\n",
    "But the benefits for your boss may come at a cost to you — both your privacy and possibly your health.\n",
    "With the global outbreak of COVID-19, your personal health could be at risk when using frequently touched screens and fingerprint scanners. The Centers for Disease Control says that coronavirus can remain on surfaces for hours, so screens and scanners should be regularly disinfected with cleaning spray or wipes. And you should wash your hands for 20 seconds or use alcohol-based hand sanitizer immediately after using one.\n",
    "In addition to these health concerns, critics argue that biometric devices pose massive personal security issues, exposing workers to potential identity theft and subjecting them to possible surveillance from corporations and law enforcement.\n",
    "In an amicus brief in a case before a federal court of appeals, a group of privacy advocates, including the ACLU and the EFF, wrote that “the immutability of biometric information” puts people “at risk of irreparable harm in the form of identity theft and/or tracking.”\n",
    "“You can get a new phone, you can change your password, you can even change your Social Security number; you can’t change your face,” said Kade Crockford, the Technology for Liberty program director at ACLU of Massachusetts.\n",
    "Companies facing legal action over their use of the machines range from fast food joints like McDonald’s and Wendy’s, to hotel chains like Marriott and Hyatt, to airlines like United and Southwest.\n",
    "In some cases, the companies have countered in the lawsuits that their employees’ union agreement allows the use of the machines: “Southwest and United contend that the plaintiffs’ unions have consented — either expressly or through the collective bargaining agreements’ management-rights clauses — and that any required notice has been provided to the unions,” the court’s opinion states.\n",
    "Other companies have not responded to requests for comment or have said they cannot comment on active litigation.\n",
    "Privacy and labor laws have lagged behind the shifts in the American workplace. But in some places, you have the right to refuse and even sue.\n",
    "\n",
    "Biometric Privacy Laws\n",
    "As the collection and use of biometrics has exploded, lawmakers in three states have responded by passing laws restricting its deployment.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' Employers are starting to use time clock machines that fingerprint employees . The machines are tied to your unique characteristics such as your face, fingerprints, how you talk, and even how you walk . The Centers for Disease Control says that coronavirus can remain on surfaces for hours .'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Import GenerativeModel from google.generativeai\n",
    "from google.generativeai import GenerativeModel\n",
    "\n",
    "# Set the API key for generative ai\n",
    "GenerativeModel.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Loading the Gemini Pro model\n",
    "model = GenerativeModel('gemini-pro')\n",
    "\n",
    "# Function to summarize the text/PDF\n",
    "def summarize_pdf(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=100)\n",
    "    chunks = text_splitter.create_documents([text])\n",
    "\n",
    "    # Define prompts for summarization\n",
    "    map_prompt_template = \"Please summarize the below pdf file:\\nfile:`{text}`\\nSummary:\"\n",
    "    final_combine_prompt_template = \"Provide a final summary of the entire pdf file with these important points.\\nAdd a Generic meaningful Title,\\nStart the precise summary with an introduction and provide the\\nsummary in number points for the pdf file.\\nfile: `{text}`\"\n",
    "\n",
    "    # Load summarization chain with prompts and model\n",
    "    summary_chain = load_summarize_chain(\n",
    "        llm=model,\n",
    "        chain_type='map_reduce',\n",
    "        map_prompt=map_prompt_template,\n",
    "        combine_prompt=final_combine_prompt_template,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # Run summarization chain on chunks of text\n",
    "    output = summary_chain.run(chunks)\n",
    "    return output\n",
    "\n",
    "# Streamlit app structure\n",
    "st.set_page_config(page_title=\"PDF Context and Summary Extractor\")\n",
    "st.header(\"PDF Context and Summary Extractor\")\n",
    "uploaded_file = st.file_uploader(\"Upload PDF file\", type=\"pdf\")\n",
    "\n",
    "if uploaded_file:\n",
    "    pdf_reader = PdfReader(uploaded_file)\n",
    "    text = \"\"\n",
    "    \n",
    "    # Extract text from each page of PDF file\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "\n",
    "    if st.button(\"Show Summary\"):\n",
    "        summary = summarize_pdf(text)\n",
    "        st.write(\"Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "from functools import reduce\n",
    "from google.generativeai import genai\n",
    "from google.generativeai import GenerationConfig\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.7,\n",
    "    top_p=1.0,\n",
    "    top_k=32,\n",
    "    candidate_count=1,\n",
    "    max_output_tokens=1024,\n",
    ")\n",
    "\n",
    "# Define model_with_limit_and_backoff function here\n",
    "\n",
    "def summarize_pdf(uploaded_file):\n",
    "    CHUNK_SIZE = 2  # number of overlapping pages\n",
    "\n",
    "    reader = PyPDF2.PdfReader(uploaded_file)\n",
    "    pages = reader.pages\n",
    "\n",
    "    initial_summary = []\n",
    "\n",
    "    for i in range(0, len(pages), CHUNK_SIZE):\n",
    "        pages_to_merge = [x for x in range(i, i + CHUNK_SIZE) if x < len(pages)]\n",
    "\n",
    "        extracted_texts = [pages[x].extract_text() for x in pages_to_merge]\n",
    "\n",
    "        text = \"\\n\".join(extracted_texts)\n",
    "\n",
    "        prompt = initial_prompt_template.format(text=text)\n",
    "\n",
    "        summary = model_with_limit_and_backoff(prompt=prompt, generation_config=generation_config).text\n",
    "\n",
    "        initial_summary.append(summary)\n",
    "\n",
    "        if pages_to_merge[-1] == len(reader.pages):\n",
    "            break\n",
    "\n",
    "    summary = reduce(initial_summary, final_prompt_template)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "# Streamlit app structure (assuming you're using Streamlit)\n",
    "st.set_page_config(page_title=\"PDF Context and Summary Extractor\")\n",
    "st.header(\"PDF Context and Summary Extractor\")\n",
    "uploaded_file = st.file_uploader(\"Upload PDF file\", type=\"pdf\")\n",
    "\n",
    "if uploaded_file:\n",
    "    if st.button(\"Show Summary\"):\n",
    "        summary = summarize_pdf(uploaded_file)\n",
    "        st.write(\"Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import backoff\n",
    "import ratelimit\n",
    "import google.generativeai as genai\n",
    "from google.generativeai import GenerativeModel\n",
    "from google.generativeai import GenerationConfig\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Loading the Gemini Pro model\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.7,\n",
    "    top_p=1.0,\n",
    "    top_k=32,\n",
    "    candidate_count=1,\n",
    "    max_output_tokens=1024,\n",
    ")\n",
    "\n",
    "CALL_LIMIT = 20  # Number of calls to allow within a period\n",
    "ONE_MINUTE = 60  # One minute in seconds\n",
    "\n",
    "# A function to print a message when the function is retrying\n",
    "def backoff_hdlr(details):\n",
    "    print(\n",
    "        \"Backing off {} seconds after {} tries\".format(\n",
    "            details[\"wait\"], details[\"tries\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "@backoff.on_exception(  # Retry with exponential backoff strategy when exceptions occur\n",
    "    backoff.expo,\n",
    "    (\n",
    "        Exception,  # Add exceptions you want to retry on here\n",
    "    ),\n",
    "    max_time=300,  # Maximum time for retries in seconds (5 minutes)\n",
    "    on_backoff=backoff_hdlr,  # Function to call when retrying\n",
    ")\n",
    "@ratelimit.limits(  # Limit the number of calls to the model per minute\n",
    "    calls=CALL_LIMIT, period=ONE_MINUTE\n",
    ")\n",
    "def model_with_limit_and_backoff(**kwargs):\n",
    "    return model.generate_content(**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "from functools import reduce\n",
    "import backoff\n",
    "import ratelimit\n",
    "import google.generativeai as genai\n",
    "from google.generativeai import GenerativeModel, GenerationConfig\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Loading the Gemini Pro model\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.7,\n",
    "    top_p=1.0,\n",
    "    top_k=32,\n",
    "    candidate_count=1,\n",
    "    max_output_tokens=1024,\n",
    ")\n",
    "\n",
    "CALL_LIMIT = 20  # Number of calls to allow within a period\n",
    "ONE_MINUTE = 60  # One minute in seconds\n",
    "\n",
    "# A function to print a message when the function is retrying\n",
    "def backoff_hdlr(details):\n",
    "    print(\n",
    "        \"Backing off {} seconds after {} tries\".format(\n",
    "            details[\"wait\"], details[\"tries\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "@backoff.on_exception(  # Retry with exponential backoff strategy when exceptions occur\n",
    "    backoff.expo,\n",
    "    (\n",
    "        Exception,  # Add exceptions you want to retry on here\n",
    "    ),\n",
    "    max_time=300,  # Maximum time for retries in seconds (5 minutes)\n",
    "    on_backoff=backoff_hdlr,  # Function to call when retrying\n",
    ")\n",
    "@ratelimit.limits(  # Limit the number of calls to the model per minute\n",
    "    calls=CALL_LIMIT, period=ONE_MINUTE\n",
    ")\n",
    "def model_with_limit_and_backoff(**kwargs):\n",
    "    return model.generate_content(**kwargs)\n",
    "\n",
    "\n",
    "def summarize_pdf(uploaded_file):\n",
    "    CHUNK_SIZE = 2  # number of overlapping pages\n",
    "\n",
    "    reader = PyPDF2.PdfReader(uploaded_file)\n",
    "    pages = reader.pages\n",
    "\n",
    "    initial_summary = []\n",
    "\n",
    "    for i in range(0, len(pages), CHUNK_SIZE):\n",
    "        pages_to_merge = [x for x in range(i, i + CHUNK_SIZE) if x < len(pages)]\n",
    "\n",
    "        extracted_texts = [pages[x].extract_text() for x in pages_to_merge]\n",
    "\n",
    "        text = \"\\n\".join(extracted_texts)\n",
    "\n",
    "        initial_prompt_template = \"Initial prompt template here\"\n",
    "        final_prompt_template = \"Final prompt template here\"\n",
    "\n",
    "        prompt = initial_prompt_template.format(text=text)\n",
    "\n",
    "        summary = model_with_limit_and_backoff(prompt=prompt, generation_config=generation_config).text\n",
    "\n",
    "        initial_summary.append(summary)\n",
    "\n",
    "        if pages_to_merge[-1] == len(reader.pages):\n",
    "            break\n",
    "\n",
    "    summary = reduce(initial_summary, final_prompt_template)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "# Streamlit app structure (assuming you're using Streamlit)\n",
    "st.set_page_config(page_title=\"PDF Context and Summary Extractor\")\n",
    "st.header(\"PDF Context and Summary Extractor\")\n",
    "uploaded_file = st.file_uploader(\"Upload PDF file\", type=\"pdf\")\n",
    "\n",
    "if uploaded_file:\n",
    "    if st.button(\"Show Summary\"):\n",
    "        summary = summarize_pdf(uploaded_file)\n",
    "        st.write(\"Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Original lanchain and vertex code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import os\n",
    "from PyPDF2 import PdfReader # For reading the PDF\n",
    "import google.generativeai as genai\n",
    "from google.generativeai import GenerativeModel\n",
    "from google.generativeai import GenerationConfig\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "\n",
    "os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Loading the Gemini Pro model\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "chunks_prompt=\"\"\"\n",
    "Please summarize the below pdf file:\n",
    "file:`{text}'\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "final_combine_prompt='''\n",
    "Provide a final summary of the entire pdf file with these important points.\n",
    "Add a Generic meaningful Title,\n",
    "Start the precise summary with an introduction and provide the\n",
    "summary in number points for the pdf file.\n",
    "file: `{text}`\n",
    "'''\n",
    "\n",
    "# Function to summarize the text/PDF\n",
    "# The model's output is treated as a condensed version of the original text, highlighting the main ideas.\n",
    "def summarize_pdf(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=100)\n",
    "    chunks = text_splitter.create_documents([text])\n",
    "    \n",
    "    map_prompt_template=PromptTemplate(input_variables=['text'],\n",
    "                                    template=chunks_prompt)\n",
    "    final_combine_prompt_template=PromptTemplate(input_variables=['text'],\n",
    "                                             template=final_combine_prompt)\n",
    "    summary_chain = load_summarize_chain(\n",
    "    llm= model,\n",
    "    chain_type='map_reduce',\n",
    "    map_prompt=map_prompt_template,\n",
    "    combine_prompt=final_combine_prompt_template,\n",
    "    verbose=False\n",
    "    )\n",
    "    output = summary_chain.run(chunks)\n",
    "    return output\n",
    "\n",
    "\n",
    "# Streamlit app structure\n",
    "st.set_page_config(page_title=\"PDF Context and Summary Extractor\")\n",
    "st.header(\"PDF Context and Summary Extractor\")\n",
    "uploaded_file = st.file_uploader(\"Upload PDF file\", type=\"pdf\")\n",
    "\n",
    "if uploaded_file:\n",
    "    pdf_reader = PdfReader(uploaded_file)\n",
    "    text = \"\"\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "\n",
    "    if st.button(\"Show Summary\"):\n",
    "        summary = summarize_pdf(text)\n",
    "        st.write(\"Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import backoff\n",
    "import pandas as pd\n",
    "import ratelimit\n",
    "from tqdm import tqdm\n",
    "import streamlit as st\n",
    "import os\n",
    "from PyPDF2 import PdfReader # For reading the PDF\n",
    "import google.generativeai as genai\n",
    "from google.generativeai import GenerativeModel\n",
    "from google.generativeai import GenerationConfig\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "initial_prompt_template = \"\"\"\n",
    "    Write a concise summary of the following text delimited by triple backquotes.\n",
    "\n",
    "    ```{text}```\n",
    "\n",
    "    CONCISE SUMMARY:\n",
    "\"\"\"\n",
    "\n",
    "final_prompt_template = \"\"\"\n",
    "    Write a concise summary of the following text delimited by triple backquotes.\n",
    "    Return your response in bullet points which covers the key points of the text.\n",
    "\n",
    "    ```{text}```\n",
    "\n",
    "    BULLET POINT SUMMARY:\n",
    "\"\"\"\n",
    "\n",
    "def summarize_pdf(uploaded_file):\n",
    "    CHUNK_SIZE = 2  # number of overlapping pages\n",
    "\n",
    "    # Read the PDF file and create a list of pages\n",
    "    reader = PyPDF2.PdfReader(pdf_file)\n",
    "    pages = reader.pages\n",
    "\n",
    "    # Create an empty list to store the summaries\n",
    "    initial_summary = []\n",
    "\n",
    "    # Iterate over the pages and generate a summary for a few pages as one chunk based on `CHUNK_SIZE`\n",
    "    for i in tqdm(range(len(pages))):\n",
    "        # Select a list of pages to merge as one chunk\n",
    "        pages_to_merge = [x for x in range(i, i + CHUNK_SIZE) if x < len(pages)]\n",
    "\n",
    "        extracted_texts = [text_from_pages[x] for x in pages_to_merge]\n",
    "\n",
    "        # Concatenate the\n",
    "        text = \"\\n\".join(extracted_texts)\n",
    "\n",
    "        # Create a prompt for the model using the concatenated text and a prompt template\n",
    "        prompt = initial_prompt_template.format(text=text)\n",
    "\n",
    "        # Generate a summary using the model and the prompt\n",
    "        summary = model_with_limit_and_backoff(prompt=prompt, generation_config=generation_config,\n",
    "        stream=True).text\n",
    "\n",
    "        # Append the summary to the list of summaries\n",
    "        initial_summary.append(summary)\n",
    "\n",
    "        # If the last page is reached, break the loop\n",
    "        if pages_to_merge[-1] == len(reader.pages):\n",
    "            break\n",
    "        \n",
    "        \n",
    "    # Use defined `reduce` function to summarize the summaries\n",
    "    summary = reduce(initial_summary, final_prompt_template)\n",
    "\n",
    "    print(summary)\n",
    "\n",
    "# Streamlit app structure\n",
    "st.set_page_config(page_title=\"PDF Context and Summary Extractor\")\n",
    "st.header(\"PDF Context and Summary Extractor\")\n",
    "uploaded_file = st.file_uploader(\"Upload PDF file\", type=\"pdf\")\n",
    "\n",
    "if uploaded_file:\n",
    "    if st.button(\"Show Summary\"):\n",
    "        summary = summarize_pdf(uploaded_file)\n",
    "        st.write(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import os\n",
    "from PyPDF2 import PdfReader  # For reading the PDF\n",
    "from dotenv import load_dotenv\n",
    "from google.generativeai import GenerativeModel\n",
    "import google.generativeai as genai\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "\n",
    "os.getenv(\"GOOGLE_API_KEY\")  # Ensure API key is present, used by LangChain internally\n",
    "\n",
    "# Set the API key for generative ai\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Loading the Gemini Pro model\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "map_combine_prompt = \"\"\"\n",
    "**Summarize each chunk (max 150 tokens):**\n",
    "file: `{chunk_text}`\n",
    "\n",
    "**Combine and refine the summaries:**\n",
    "Add a title based on the key points...\n",
    "Start with an introduction summarizing the main points...\n",
    "List the main points in bullet points...\n",
    "file: `{entire_text}`\n",
    "\"\"\"\n",
    "\n",
    "# Streamlit app structure\n",
    "st.set_page_config(page_title=\"PDF Context and Summary Extractor\")\n",
    "st.header(\"PDF Context and Summary Extractor\")\n",
    "uploaded_file = st.file_uploader(\"Upload PDF file\", type=\"pdf\")\n",
    "\n",
    "if uploaded_file:\n",
    "    pdf_reader = PdfReader(uploaded_file)\n",
    "    text = \"\"\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "\n",
    "    if st.button(\"Show Summary\"):\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    "        chunks = text_splitter.create_documents([text])\n",
    "\n",
    "        map_prompt_template = PromptTemplate(input_variables=[\"chunk_text\"], template=map_combine_prompt)\n",
    "        # Configure LangChain with Gemini Pro LLM\n",
    "        summary_chain = load_summarize_chain(\n",
    "            chain_type=\"map_reduce\",\n",
    "            map_prompt=map_prompt_template,\n",
    "            llm= model,  # Explicitly specify Gemini Pro model\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        # Run the summarized chain with extracted text and prompts\n",
    "        output = summary_chain.run(chunks)\n",
    "\n",
    "        # Clean and print the final summary\n",
    "        final_summary = \"\\n\".join(output[\"combined_output\"][0].splitlines())\n",
    "        st.write(\"Summary:\", final_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
